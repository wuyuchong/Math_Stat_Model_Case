---
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    template: template_all.tex
  word_document: default
classoption: "hyperref,"
geometry: margin=1in
csl: chinese-gb7714-2005-numeric.csl
bibliography: reference.bib
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{indentfirst}
   - \setlength{\parindent}{4em}
logo: "cufe.jpg"
nocite: | 
  @吴林武, @代倩宇, @苏秦
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.pos = 'H', echo = FALSE, warning = FALSE, message = FALSE)
library(knitr)
library(tidyverse)
library(caret)
library(kernlab)
library(pROC)
library(kableExtra)
library(e1071)
# base_family = 'STXihei'
```

# 摘要{-}

我们通过案例研究分析某电商消费者购买行为数据，对消费者购买意向进行预测，通过建立多种算法模型，预测每个**消费者购买的概率**，并进行模型效果的比较。找到最具有普适性的预测模型，进而有效地推广到各种电商商品的销售，为各类型的销售者提供营销模式的参考依据。

我们在保持重抽样方法相同的情况下，使用多种常用机器学习模型对样本进行训练，与此同时，使用 10 折**交叉验证**进行模型比较。最终，**PLSDA**模型在准确率和 Kappa 两项评判指标下都具有最好的效果。

在变量选择上，最重要的变量为：**网页价值**、**信息页访问时长**和**商品页访问时长**。对于所有的用户，这三个特征具有普适性，在预测用户是否购买中是较为重要的衡量因素。

\ 

\ 

\ 

\ 

关键词： **购买行为**， **分类预测模型**， **机器学习**， **变量选择**， **模型比较**

\ 

\ 

\ 

\ 

\setcounter{tocdepth}{2}
\tableofcontents

\newpage

# 背景

近年来，中国的电子商务快速发展，交易额连创新高，电子商务在各领域的应用不断扩展和深化、相关服务业蓬勃发展、支撑体系不断健全完善、创新的动力和能力不断增强。电子商务正与实体经济深度融合，进入规模性发展的阶段，对经济社会生活的影响不断增大，正成为我国经济发展的新引擎。整个社会的消费模式都因此产生了很大变化，从以实体店购物为主转变为足不出户的网络购物方式。2015年，中国电子商务市场交易规模达16.4万亿元，增长22.7%。其中网络购物增长36.2%，成为推动电子商务市场发展的重要力量。网络购物以其便利的操作方式、短时间等优势逐步成为居民购物的主要方式，目前仍维持着快速发展的趋势。线上购物凭借其庞大的客户群体且不断增长的购买方式，占B2B（企业对企业）、B2C（企业对个人）和C2C（个人对个人）市场收入的很大一部分。

据中商产业研究院整理，2019年天猫“双十一”全天成交额为2684亿元，超2018年549亿元，再次创下新纪录。消费形式的转变是全球化趋势，据2018年Optinmonster公司的调查数据 ^[https://optinmonster.com/online-shopping-statistics/] 可知，有69％的美国人每月都在网上购物，而25％的美国人每月至少一次在网上购物。仅在美国，预计2023年将有3亿在线购物者，占全国人口的91％，且电子商务零售购买量预计将从14.1％上升到22％。

在现今互联网大数据时代，随着电商的快速发展，分析用户购买意向数据对电商平台商品销量预测、确定商品营销范围，挖掘潜在用户等方面均有重要意义。影响用户购买意向的因素有很多，网页的访问数和访问时长、网页的访问时间和形式、线上购买时所用的操作系统和浏览器甚至也会对线上购物产生一定的作用。通过这些信息，企业和个人销售者可以了解和分析购物者线上购买特定商品的具体行为习惯以及各种外部因素以何种方式影响着商品销售。因此，销售者可以通过各影响因素之间的统计关系进行数据分析预测商品销量，合理安排市场营销方式，挖掘更多的潜在用户，进一步增加他们的销售和收入。

# 文献综述

不同学者通过不同的研究方法对用户的线上购买影响因素进行了分析。

袁和林等采用偏最小二乘方法通过建立 PLR-SEM 模型研究了顾客网购行为影响因素。通过分析发现个人属性的影响要强于电子服务因素。 @Yuan2016What

李宝库等通过回归分析、因子分析以及方差分析等方法研究了用户线上购买意向的影响因素。在用户网购行为的影响因素中，每个影响因素作用的大小不同，因此需要通过构建用户网购行为影响因素模型进而确定每个影响路径对应的系数。 @李宝库

金灏利用分类与预测算法分析了用户的浏览行为和购买行为，实现了对潜在用户的挖掘。利用电商企业网站数据以及国内前期研究资料对本文所提出的数据挖掘处理计算方法进行实证模拟，研究企业如何实现对潜在客户相关信息的挖掘，促进潜在客户转变为企业的现实客户、忠实客户。

# 研究方法

1. 首先对数据进行分析和处理，建立模型对消费者购买意向进行预测。其次，选用多种机器学习模型进行模型比较，为研究影响消费者购买意向的因素提供更多的评判思路。找到最具有普适性的预测模型，进而有效地推广到各种电商商品的销售，为各类型的销售者提供营销模式的参考依据。

2. 我们通过应用多种机器学习模型如Logit回归、线性判别分析（LDA）、偏最小二乘判别分析（PLSDA）、SVM、随机梯度助推法等方法，探究影响消费者购买意向的因素，将重要的变量筛选出来，理清楚其影响关系，使用数据集中的这些变量预测商品销售。并且将数据向更深的层次进行挖掘，探究内在的关系。

3. 通过分析各变量之间的关系，找出变量之间是否有相关性，提高模型的准确性。最后通过数据可视化的方式，利用各种图表将变量之间存在的联系直观的展现出来。

4. 该研究通过分析消费者购买意向的影响因素为商品销售的预测提供思路，可以根据实际情况加以调整。


# 数据集与描述分析

## 数据集说明

我们使用一个公开的数据集 ^[数据来源: http://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset#] ，它有 35 个变量，310 个观测。 ^[模型的变量取值和分布见附录]

数据集由分属于12330个会话的特殊向量组成。在数据集的12330个会话中，其中84.5%（10422）是以购物结束的负类样本，其余（1908）是以购物结束的正类样本。数据集的形成使得每个会话在一年的时间内属于不同的用户，以避免出现特定活动、特殊日期、用户配置文件或时段的趋势。

数据集由10个数值属性和8个分类属性组成。“Revenue”特征可用作类标签。“Administrative”、“AdministrativeDuration“、“Informational”、“Informational Duration”、“Product Related” 和“Product Related Duration”特征代表访问者在该会话中访问的不同类型页面的数量以及在这些不同类别的页面中花费的总时间。这些特征的值来源于用户访问的页面的URL信息，并在用户进行实际操作时实时更新，例如从一个页面移到另一个页面。“Bounce Rate”、“Exit Rate”、“Page Value”代表了“谷歌分析”对电子商务网站中每个页面的度量。Bounce Rate的值是在该会话期间从该页面进入站点然后离开（“跳出”）而不触发对分析服务器的任何其他请求的访问者的百分比。特定网页的Exit Rate的值是该页的所有页面浏览量，即会话中最后一个页面的百分比。Page Value是用户在完成电子商务交易之前访问的网页的平均值。Special Day是指网站访问时间接近某个特定的日子（如母亲节、情人节），在这一天，会议更有可能最终完成交易。此属性的值是通过考虑电子商务的动态（如订单日期和交货日期之间的持续时间）来确定的。例如，对于情人节，该值在2月2日和2月12日之间取一个非零值，在此日期之前和之后为零，除非它接近另一个特殊的日期，否则它的最大值出现在2月8日值为1。数据集还包括操作系统、浏览器、区域、流量类型、访客类型（返回或新访客）、布尔值，指示访问日期是周末还是一年中的月份。

```{r}
table = read.csv("dictionary.csv", header = TRUE)
names(table) = c("变量名", "变量描述", "数据格式")
kable(table, booktabs=TRUE, format="latex", caption = "变量解释和类型") %>% 
  kable_styling(latex_options=c("scale_down", "HOLD_position"))
```

```{r}
dat = read.csv("data.csv", header = TRUE)
dat$Month = as.factor(dat$Month)
dat$OperatingSystems = as.factor(dat$OperatingSystems)
dat$Browser = as.factor(dat$Browser)
dat$Region = as.factor(dat$Region)
dat$TrafficType = as.factor(dat$TrafficType)
dat$VisitorType = as.factor(dat$VisitorType)

dat$Weekend = ifelse(dat$Weekend == TRUE, 1, 0)
dat$Weekend = as.factor(dat$Weekend)
dat$Revenue = ifelse(dat$Revenue == TRUE, 1, 0)
dat$Revenue = as.factor(dat$Revenue)
```

## 数据预处理

1. 对原始数据进行去重补缺等预处理。
1. 我们将用户被转化**购买**，与**未购买**相对应，生成一个虚拟变量。

\newpage

## 描述分析

### 跳出率/退出率

```{r}
dat_0 = dat %>% 
  filter(Revenue == 0) %>% 
  slice(1: 10000)
dat_1 = dat %>% 
  filter(Revenue == 1) %>% 
  slice(1: 10000)
dat_01 = rbind(dat_0, dat_1)
```


```{r fig.align="center", fig.cap="未购买与购买两类用户跳出率、退出率分布密度图（红色代表购买）", out.width="60%"}
ggplot(dat_01, aes(x = BounceRates, y = ExitRates, color = Revenue)) +
  geom_jitter(alpha = 0.3) + 
  theme_minimal() +
  scale_x_continuous(label = scales::percent) +
  scale_y_continuous(label = scales::percent) +
  labs(x = "Bounce Rates", y = "Exit Rates") +
  geom_rug(sides = "bl") +
  scale_color_manual(values = c("#037418", "red"))
```

购买的用户跳出率、退出率均较低，集中在0-5%之间。未购买的用户的跳出率、退出率分布较分散，大部分集中在0-10%之间。跳出率、退出率是是否购买的一个较为重要的衡量指标。

### 周末、网页价值与购买行为

```{r fig.align="center", fig.cap="周末网页价值分布密度图（红色代表购买）", out.width="60%"}
dat %>% 
  filter(PageValues < 75) %>% 
  ggplot(aes(x = Weekend, y = PageValues, fill = as.factor(Revenue))) +
    geom_violin(alpha = 0.3) + 
    theme_minimal() +
    labs(fill = "Revenue", y = "Page Values", caption = "Exclude Page Values > 75") +
    scale_fill_manual(values = c("#037418", "red"))
```

由图可知，无论用户最后是否购买，用户是否在周末操作网页价值差别不大。我们认为周末这一特征对用户的购买影响较小，网页价值与用户购买的相关性也较低。

### 临近特殊日商品页访问与购买行为

```{r fig.align="center", fig.cap="购买与不购买两类用户临近特殊日商品页访问数箱线图（红色代表购买）", out.width="60%"}
dat %>% 
  filter(ProductRelated < 300) %>% 
  ggplot(mapping = aes(x = as.factor(SpecialDay), y = ProductRelated, fill = Revenue)) +
    geom_boxplot(alpha = 0.5) +
    labs(x = "Special Day", y = "Product Related", fill = "Revenue", caption = "Exclude Product Related > 300") +
    theme_minimal() +
    scale_fill_manual(values = c("#037418", "darkred"))
```

首先，购买的用户商品页访问数高于不购买的用户。用户访问更多的商品页代表着用户有更强烈的购买需求或购买欲望。越接近特殊日期，无论用户最后最终是否购买，离群值均显著增加，说明用户越接近特殊日期，访问的商品页会显著增多。因此，特殊日期是促进用户购买的一个重要因素。

### 临近特殊日的用户类型分布

```{r fig.align="center", fig.cap="临近特殊日的用户类型分布（红色代表购买）", out.width="60%"}
ggplot(dat, aes(x = as.factor(SpecialDay), fill = VisitorType)) +
  geom_bar(stat = "count", position = "fill") +
  theme_minimal() +
  labs(y = "distribution", x = "Special Day") + 
  coord_flip()
```

由图可知：

1. 回访者的贡献度要远远高于其他两种访问者。
2. 接近特殊日期，新用户的贡献度增加。
3. 其他类型的访问者贡献度几乎为0。

可见是否是回访者是衡量用户是否购买的重要因素之一。

# 建立解释模型

## 拟合

我们将**购买**作为响应变量，建立 logit 回归模型。 选取的自变量有：

主页访问数
/ 主页访问时长
/ 信息页访问数
/ 信息页访问时长
/ 商品页访问数
/ 商品页访问时长
/ 跳出率
/ 退出率
/ 网页价值
/ 特殊日
/ 月份

我们将因子型变量转换成隐变量后加入模型中，连续型变量直接加入模型。

```{r}
logit2 = glm(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = dat, family = binomial(link = "logit"))
logit2_sum = summary(logit2)
```

根据 Z 检验的 p 值可知 ^[模型详细见附录] ，“商品页访问时长”、“退出率”、“网页价值”以及月份中的“二月”、“三月”、“五月”、“十一月”和“十二月”在统计上显著。 ^[在 95% 置信区间下] 

退出率高的用户购买的可能性较低。退出率是指，对于某一特定页面而言，从该页面离开网站的访问占所有浏览到该页面的访问的百分比。用户从该页面退出的比率越高，可能意味着用户对该页面商品的兴趣度越低，因此购买的概率也随之降低。


## 预测

我们划分 四分之三 的训练集和 四分之一 的验证集。

```{r}
set.seed(1)
inTraining <- createDataPartition(dat$Revenue, p = .75, list = FALSE)
train <- dat[inTraining,]
test <- dat[-inTraining,]
```

```{r fig.align="center", fig.cap="预测的购买概率值（红色代表购买）", out.width="70%", fig.width=5, fig.height=2}
logit3 = glm(Revenue ~ ., data = train, family = binomial(link = "logit"))
probability = predict(logit3, test, type = "response")
distribution = as.data.frame(probability)
distribution = cbind(distribution, group = test$Revenue)
ggplot(distribution, aes(x = probability, fill = group)) +
  geom_density(alpha = 0.3) + 
  theme_minimal() +
  scale_fill_manual(values = c("#037418", "darkred"))

testPred = probability
testPred[testPred > 0.5] = 1
testPred[testPred <= 0.5] = 0
testPred = as.factor(testPred)
```

从预测概率分布图可知，对于未购买的用户，我们预测出的购买概率值比较低，但对于购买的用户，预测出的购买概率值比较分散，超过一半的预测购买率低于50%。

我们猜想：**模型讲未购买的用户预测为购买的用户的概率比较地，因此比较难识别出购买的用户。**

## 混淆矩阵与验证结果

我们将预测概率大于 50% 的判定为购买。

灵敏度（Sensitivity）

$$\text{灵敏度} = \frac{\text{正确判定为“购买”的样本数量}}{\text{观测到的“购买”的样本数量}}$$

特异度（Specificity）

$$\text{特异度} = \frac{\text{正确判定为“购买”的样本数量}}{\text{观测到的“购买”的样本数量}}$$

假购买率

$$\text{假购买率} = 1 - \text{观测到的“购买”的样本数量}$$

```{r}
confusion = confusionMatrix(data = testPred,
                reference = test$Revenue,
                positive = "1")
kable(as.data.frame(confusion$table), format="latex", booktabs=TRUE, caption = "混淆矩阵表") %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
table = as.data.frame(confusion$overall)
names(table) = c("指标值")
table = t(table)
rownames(table) = NULL
kable(table, booktabs=TRUE, format="latex", caption = "验证结果表", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
table = as.data.frame(confusion$byClass[1:5])
names(table) = c("指标值")
table = t(table)
kable(table, format="latex", booktabs=TRUE, caption = "灵敏度和特异度等指标表", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

使用 Logit 回归模型进行预测的准确率大致为: 88.4% ，准确率较高。

由灵敏度可知，40% 的有购买倾向的顾客会被模型成功捕捉到；由特异度可知，模型的误判率只有2.7%。模型可以捕捉到购买的顾客，同时模型预测认为会购买的顾客有极大的概率会进行购买。

## 接受者操作特征（ROC）曲线

我们使用 ROC 曲线 (Altman 和 Bland 1994; Brown 和 Davis 2006; Fawcett 2006)  决定分类概率的阈值。 @Altman1994Diagnostic @Brown2006Receiver @Fawcett2006An

```{r fig.align="center", fig.cap="Logit 模型的 ROC 曲线", out.width="60%"}
rocCurve = roc(response = test$Revenue,
               predictor = probability,
               levels = rev(levels(test$Revenue)),
               plot = TRUE,
               print.thres=TRUE, print.auc=TRUE)
```

通过降低阈值可以达到提高灵敏度的目的，但同时也承担着特异度降低的风险，导致误判率上升。在实际操作中，可以通过使用不同阈值的方法达到不同的效果：

1. 当业务要求尽可能减低误判率时，则可以选择适当提高阈值以达到目的。
1. 当业务要求尽可能识别出会购买的用户时，则可以选择适当降低阈值以达到目的。


# 预测模型的选择

## 抽样、训练与评价指标

```{r cache=TRUE}
set.seed(1)
inTraining <- createDataPartition(dat$Revenue, p = .10, list = FALSE)
training <- dat[inTraining,]
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated two times
                           repeats = 2)
```

我们使用 Kappa 统计量（Cohen 1960）作为模型准确度的度量指标。 @Cohen1960A 

$$\mathrm{Kappa}=\frac{O-E}{1-E}$$

上述公式中，O代表准确性，E则代表的是根据混淆矩阵边缘计数得出的期望准确性。1值表示模型的预测与观测类是相同的，0值意味着观测类和预测类是不同的，该统计量取值是在-1和1之间，其中负数代表实际和预测值是相反的，但实际情况中，绝对值较大的负数值在模型的预测中出现的频率非常低。在各类分布相同的时，总精确度与 Kappa成比例。Kappa值在0.30到0.50之间，代表一致性合理，但这一取值区间也要依具体情况而定。（Agresti 2002）

## Logit 回归

```{r}
set.seed(1)
logit <- train(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = training, 
                 method = "glm", 
                 trControl = fitControl)
```

```{r}
table = logit$results
rownames(table) = NULL
kable(table, format="latex", booktabs=TRUE, caption = "在重抽样下 Logit 模型的表现", digits = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

Logit的应用非常广泛，因为该模型非常简单，并且计算速度很快，而且具有很强的可解释性。尽管逻辑回归模型的预测分类能力较好，但如果我们仅着重于预测准确性这一衡量指标，可以找到表现更好的模型。

## 线性判别分析（LDA）

我们使用 Fisher（1936）@fisher36lda 和 Welch（1939）@WELCH1939 提出的最优判别准则的方式。

通过贝叶斯法则，已知：

$$
\operatorname{Pr}\left[Y=C_{\ell} | X\right]=\frac{\operatorname{Pr}\left[Y=C_{\ell}\right] \operatorname{Pr}\left[X | Y=C_{\ell}\right]}{\sum_{\ell=1}^{C} \operatorname{Pr}\left[Y=C_{\ell}\right] \operatorname{Pr}\left[X | Y=C_{\ell}\right]}
$$

若：

$$
\operatorname{Pr}\left[Y=C_{1}\right] \operatorname{Pr}\left[X | Y=C_{1}\right]>\operatorname{Pr}\left[Y=C_{2}\right] \operatorname{Pr}\left[X | Y=C_{2}\right]
$$

将 X 分入 $C_1$，得到线性判别函数为：

$$
X^{\prime} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{\ell}-0.5 \boldsymbol{\mu}_{\ell}^{\prime} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{\ell}+\log \left(\operatorname{Pr}\left[Y=C_{\ell}\right]\right)
$$

```{r cache=TRUE}
set.seed(1)
lda <- train(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = training, 
                 method = "lda", 
                 trControl = fitControl,
               preProc = c("center", "scale"))
```

```{r fig.align="center", fig.cap="在重抽样下 LDA 模型的准确率分布", out.width="50%"}
trellis.par.set(caretTheme())
densityplot(lda, pch = "|")
```

## 偏最小二乘判别分析（PLSDA）

当变量之间有较强的多重共线性，LDA模型便不再适用。我们尝试通过使用主成分分析来压缩变量空间的维度。这一方法的缺点是，PCA可能无法识别能将样本分类的较好的变量组合，同时，由于PCA是无监督学习，我们很难通过它找到一个最优的分类预测。

Berntsson 和 Wold（1986） @Peder1986Comparison 提出了偏最小二乘判别分析（PLSDA）。尽管 Liu 和 Rayens（2007） @Liu2007PLS 指出，在不降维的情况下，LDA 一定优于 PLS。但降维后 PLS 的表现可能超过 LDA。

```{r cache=TRUE}
set.seed(1)
plsda <- train(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = training, 
                 method = "pls", 
                 trControl = fitControl,
               tuneGrid = expand.grid(.ncomp = 1:10))
```

```{r fig.align="center", fig.cap="Kappa 指标和准确率随主成分个数的变化", out.width="49%", fig.show='hold'}
trellis.par.set(caretTheme())
plot(plsda, metric = "Kappa")
plot(plsda)
```

由图可知， 随主成分个数的增多，Kappa 指标先上升，之后稍有下降；随着主成分个数的增加，准确率先下降，后上升到顶峰、再下降。在此模型中，对于Kappa指标和准确率指标，选取前 4 个主成分都是最优的。

```{r fig.align="center", fig.cap="变量重要程度", out.width="50%", fig.width=5, fig.height=2}
plsImp = varImp(plsda, scale = FALSE)
table = data.frame(variables = rownames(plsImp$importance), importence = plsImp$importance$Overall)
table = table %>% 
  arrange(desc(importence)) %>% 
  top_n(8)
ggplot(table, aes(x = reorder(variables, importence), y = importence)) +
  geom_col() +
  theme_minimal() +
  coord_flip() +
  labs(x = "variables")
```

变量以在 PLSDA 模型中的重要性为标准进行排序，重要度排名前三位的分别是：Pagevalue网页价值，Informational Duration信息页访问时长，ProductRelated Duration商品页访问时长。**对于所有的用户，这三个特征具有普适性，在预测用户是否购买中是较为重要的衡量因素。**

而重要程度最低的三个变量分别是MonthJul、MonthDec、MonthJune。这三个变量对用户购买商品没有太大的影响。


## SVM

```{r cache=TRUE}
set.seed(1)
svm <- train(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = training, 
                 method = "svmRadial", 
                 trControl = fitControl,
            tuneLength = 5)
```

```{r fig.align="center", fig.cap="调优参数不同取值下的准确率和 Kappa 指标变化", out.width="49%", fig.show='hold'}
trellis.par.set(caretTheme())
plot(svm)
plot(svm, metric = "Kappa")
```

在损失参数增大的同时，准确率指标与 Kappa 指标的变化趋势相同，准确率和 Kappa 值均呈现上升趋势。

## 随机梯度助推法（GBM）

我们使用 Friedman等（2000） @Ben2000Tissue 提出的通过最小化指数损失函数实现分类的方式，构建随机梯度助推模型。

$$
f_{i}^{(0)}=\log \frac{\hat{p}}{1-\hat{p}}
$$

其中，$f(x)$ 为预测值，$\hat{p}_{i}=\frac{1}{1+\exp [-f(x)]}$

```{r cache=TRUE, include=FALSE}
set.seed(1)
gbm <- train(Revenue ~ Administrative + Administrative_Duration + Informational + Informational_Duration + ProductRelated + ProductRelated_Duration + BounceRates + ExitRates + PageValues + SpecialDay + Month, data = training, 
                 method = "gbm", 
                 trControl = fitControl)
```

```{r fig.align="center", fig.cap="调优参数和迭代次数不同取值下的准确率和 Kappa 指标变化", out.width="49%", fig.show='hold'}
trellis.par.set(caretTheme())
plot(gbm)

trellis.par.set(caretTheme())
plot(gbm, metric = "Kappa")
```

当迭代次数为1次和2次时，随着助推树的加深，Kappa值和准确率均呈现先上升后下降的趋势。迭代两次的Kappa值和准确率高于迭代一次的Kappa值和准确率。但迭代次数为3次时，随着树的加深，Kappa值和准确率呈下降趋势。

```{r fig.align="center", fig.cap="在重抽样下 GBM 模型的准确率分布", out.width="50%"}
trellis.par.set(caretTheme())
densityplot(gbm, pch = "|")
```

## 模型间的比较

所有模型都使用相同的重抽样方法，且我们保证不同模型使用的重抽样样本完全一致。

```{r}
resamp = resamples(list(LDA = lda, PLSDA = plsda, SVM = svm, GBM = gbm, Logit = logit))
s1 = summary(resamp)
s2 = summary(diff(resamp))
```

```{r fig.align="center", fig.cap="模型间准确率和 Kappa 的比较（0.95 置信区间）", out.width="49%", fig.height=3, fig.width=6, fig.show='hold'}
ggplot(resamp,
       models = c("LDA", "PLSDA", "GBM", "Logit"),
       metric = "Kappa",
       conf.level = 0.95) +
  theme_bw()

ggplot(resamp,
       models = c("LDA", "PLSDA", "SVM", "GBM", "Logit"),
       metric = "Accuracy",
       conf.level = 0.95) +
  theme_bw()
```

若由 **Kappa** 值来衡量，GBM模型效果最好，Logit 和LDA 模型次之，PLSDA 模型效果最差。

若由**准确率**来衡量，从偏差的角度来看，GBM模型效果最好，SVM 模型次之；从方差的角度来看，GBM模型、SVM 模型方差较小，PLSDA模型和logit模型方差较大。

\newpage

# 结论

在此研究中，我们主要研究了电商用户的购买行为。我们通过这个研究一个案例，所得出的相关结论，对电商平台的营销决策提供一定的建议。

于此同时，此研究建立的多种预测购买模型，完全可以在电商的其它领域中适当地调整后加以应用。

## 变量解释

1. 购买的用户跳出率、退出率均较低，集中在0-5%之间。未购买的用户的跳出率、退出率分布较分散，大部分集中在0-10%之间。跳出率、退出率是是否购买的一个较为重要的衡量指标。
1. 无论用户最后是否购买，用户是否在周末操作网页价值差别不大。我们认为周末这一特征对用户的购买影响较小，网页价值与用户购买的相关性也较低。
1. 购买的用户商品页访问数高于不购买的用户。用户访问更多的商品页代表着用户有更强烈的购买需求或购买欲望。越接近特殊日期，无论用户最后最终是否购买，离群值均显著增加，说明用户越接近特殊日期，访问的商品页会显著增多。因此，特殊日期是促进用户购买的一个重要因素。
1. 回访者的贡献度要远远高于其他两种访问者。接近特殊日期，新用户的贡献度增加。其他类型的访问者贡献度几乎为0。可见是否是回访者是衡量用户是否购买的重要因素之一。

## 模型选择

若由 **Kappa 值**来衡量，GBM模型效果最好，Logit 和LDA 模型次之，PLSDA 模型效果最差。

若由**准确率**来衡量，从偏差的角度来看，GBM模型效果最好，SVM 模型次之；从方差的角度来看，GBM模型、SVM 模型方差较小，PLSDA模型和logit模型方差较大。

综合来看，**GBM 模型**具有最好的效果。

## 变量选择

在 PLSDA 模型中的各变量重要性排序：排在前三名的是Pagevalue网页价值，Informational Duration信息页访问时长，ProductRelated Duration商品页访问时长。它们是在预测用户是否购买中是较为重要的衡量因素。而月份变量重要性较低，对用户购买商品没有太大的影响。

\newpage

# 参考文献

<div id="refs"></div>

\newpage

# 附录

## 模型间准确率和 Kappa 的比较

```{r}
kable(s1$statistics$Accuracy, format="latex", booktabs=TRUE, caption = "模型间准确率的比较", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
kable(s2$table$Accuracy, format="latex", booktabs=TRUE, caption = "模型间准确率差异矩阵", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
kable(s1$statistics$Kappa, format="latex", booktabs=TRUE, caption = "模型间 Kappa 的比较", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
kable(s2$table$Accuracy, format="latex", booktabs=TRUE, caption = "模型间Kappa差异矩阵", digit = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

## 模型训练指标详情

```{r}
table = lda$results
rownames(table) = NULL
kable(table, format="latex", booktabs=TRUE, caption = "在重抽样下 LDA 模型的表现", digits = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
table = plsda$results
rownames(table) = NULL
kable(table, format="latex", booktabs=TRUE, caption = "在重抽样下 PLSDA 模型的表现", digits = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
kable(svm$results, format="latex", booktabs=TRUE, caption = "在重抽样下 SVM 模型的表现", digits = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
kable(gbm$results, format="latex", booktabs=TRUE, caption = "在重抽样下 GBM 模型的表现", digits = 3) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

## Logit 回归结果

```{r}
kable(logit2_sum$coefficients, format="latex", booktabs=TRUE, caption = "Logit回归系数表", digit = 2) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```

```{r}
logit2_sum
```

## 数据指标明细

```{r}
str(dat)
```

```{r}
summary(dat)
```



